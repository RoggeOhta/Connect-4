---
sidebar_position: 2
---

# 对弈模型优化

官方的对弈模型采用的是基于pytorch[^1]训练的DQN[^2]网络，以ONNX[^3]形式导出使用。

不过你也可以训练自己的对弈模型，你可以使用任何你喜欢的框架训练。但是模型文件必须要导出为ONNX格式。

模型输入一个矩阵形态的棋盘状态，输出6个位置的决策值，其中决策值最高的将会作为下一次机械臂的下棋位置。

[^1]: PyTorch是一个开源的机器学习框架，主要用于构建深度神经网络模型。它提供了丰富的工具和库，用于处理各种机器学习任务，包括计算机视觉、自然语言处理、图像和语音识别等。PyTorch采用动态图计算的方式，使得模型的构建和调试更加灵活和直观。

[^2]: DQN是一种强化学习算法，全称为Deep Q-Network。它结合了深度神经网络和Q-learning算法，用于解决强化学习问题。DQN在强化学习领域中取得了重要的突破，特别是在处理具有高维状态空间的任务时，如游戏玩法。

[^3]: ONNX是开放神经网络交换（Open Neural Network Exchange）的缩写，是一个开源的跨平台机器学习模型交换格式。它允许用户在不同的深度学习框架之间无缝迁移模型。ONNX可以将深度学习模型导出为一个独立的文件，使得模型可以在不同的框架（如PyTorch、TensorFlow等）或硬件平台上进行部署和运行，提高了模型的可移植性和灵活性。
